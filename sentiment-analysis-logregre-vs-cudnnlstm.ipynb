{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment-analysis-logregre-vs-cudnnlstm', 'amazonreviews', 'glovetwitter100d']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import bz2\n",
    "import gc\n",
    "import chardet\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "0a2b61717e468b5a6b9820240af45236260c7168"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Conv1D, GlobalMaxPool1D, Dropout, concatenate, Layer, InputSpec, CuDNNLSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "from keras import activations, initializers, regularizers, constraints\n",
    "from keras.utils.conv_utils import conv_output_length\n",
    "from keras.regularizers import l2\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "400fa34d9a714a22a20259ba3140a030551e1986"
   },
   "source": [
    "# Read & Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File('../input/amazonreviews/train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('../input/amazonreviews/test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df4ce91d1b19fe5c93e401a3ce64fd4d1f07787a"
   },
   "source": [
    "## Create Lists containing Train & Test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "d554f2ba-0eeb-4764-b1df-febfbdda5c2c",
    "_uuid": "93213b70d17edcdeeba304a7344e415733d0ce17"
   },
   "outputs": [],
   "source": [
    "train_file_lines = train_file.readlines()\n",
    "test_file_lines = test_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "4a67bd8e9d198026184e07764feeaa8727288e64"
   },
   "outputs": [],
   "source": [
    "del train_file, test_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0357f1f5175f121b2d2435fd7e2477f4b2a0e17e"
   },
   "source": [
    "## Convert from raw binary strings to strings that can be parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "ceda406d-42f3-46de-930c-f658b1090792",
    "_uuid": "5758abb69d2f0b7434041d5444916089ef468d88"
   },
   "outputs": [],
   "source": [
    "train_file_lines = [x.decode('utf-8') for x in train_file_lines]\n",
    "test_file_lines = [x.decode('utf-8') for x in test_file_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "dcc69630-91ef-4a06-9eb4-56c4b0390a49",
    "_uuid": "539afb8a439f11442ad8ca5c1de042a3a310462f"
   },
   "outputs": [],
   "source": [
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_file_lines]\n",
    "\n",
    "for i in range(len(train_sentences)):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "    \n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_file_lines]\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "                                                       \n",
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "a1b5fb60ceafd2f0776af45a4722fc55c15903c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stuning even for the non-gamer: this sound track was beautiful! it paints the senery in your mind so well i would recomend it even to people who hate vid. game music! i have played the game chrono cross but out of all of the games i have ever played it has the best music! it backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. it would impress anyone who cares to listen! ^_^'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_file_lines, test_file_lines\n",
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "03b49435-b06b-4f72-b8f2-27faae8cf478",
    "_kg_hide-input": false,
    "_uuid": "3754d4d288fe5411ee0cc26112a17dd34a6e427c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test dataframes\n",
    "#Na_train = {'Sentence': train_sentences, 'Label': train_labels}\n",
    "#Nav_train = pd.DataFrame(Na_train)\n",
    "\n",
    "#Na_test = {'Sentence': test_sentences, 'Label': test_labels}\n",
    "#Nav_test = pd.DataFrame(Na_test)\n",
    "\n",
    "#Nav_train.head()\n",
    "#Nav_train.to_csv(\"amazon_train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Positive and Negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pos = Nav_train[Nav_train['Label'] == 1]\n",
    "#train_pos = Nav_train['Sentence']\n",
    "#train_neg = Nav_train[Nav_train['Label'] == 0]\n",
    "#train_neg = Nav_train['Sentence']\n",
    "\n",
    "#test_pos = Nav_test[Nav_test['Label'] == 1]\n",
    "#test_pos = Nav_test['Sentence']\n",
    "#test_neg = Nav_test[Nav_test['Label'] == 0]\n",
    "#test_neg = Nav_test['Sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Cleaning and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Using Logistic Regression\n",
    "\n",
    "## Vectorization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(binary = True)\n",
    "cv.fit(train_sentences)\n",
    "log_train=cv.transform(train_sentences)\n",
    "log_test =cv.transform(test_sentences)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "#for c in [0.01, 0.05, 0.025, 0.5, 1]:\n",
    "#lr = LogisticRegression(C=0.05)\n",
    "#lr.fit(log_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "#pickle.dump(lr, open(filename, 'wb'))\n",
    " \n",
    "# some time later...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# load the model from disk\n",
    "lr_model = pickle.load(open(\"../input/sentiment-analysis-logregre-vs-cudnnlstm/finalized_model.sav\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The product is  good: RESULT:\n",
      "[1]\n",
      "The product is not good: RESULT:\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"The product is  good: RESULT:\")\n",
    "testing = \"the product is  good\"\n",
    "testing = [testing]\n",
    "print(lr_model.predict(cv.transform(testing)))\n",
    "\n",
    "print(\"The product is not good: RESULT:\")\n",
    "testing = \"The product is not good\"\n",
    "testing = [testing]\n",
    "\n",
    "print(lr_model.predict(cv.transform(testing)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using LogisticRegression is 0.90634\n"
     ]
    }
   ],
   "source": [
    "acc=(accuracy_score(test_labels, lr_model.predict(log_test)))\n",
    "print(\"Accuracy using LogisticRegression is\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: Using CuDNNLSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "0ab720adb979acbb71bdbc1b0baf975b592049ea"
   },
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "7fafbeb9c7604d9306ee0b11831769b616bb3b49"
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "c9ebc89a3e784567de70e9b77b514e0553e75a43"
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "a228c36b8fad3cf02e19cbb43210bb10735807a2"
   },
   "outputs": [],
   "source": [
    "tokenized_train = tokenizer.texts_to_sequences(train_sentences)\n",
    "X_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "3d129cf469aab9863fe98e96eae08c669547377d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74, 11, 1, 610, 6395, 8, 176, 485, 13, 364, 6, 6071, 1, 10, 60, 438, 27, 69, 3, 39, 1835, 6, 74, 5, 135, 72, 678, 16703, 145, 119, 3, 20, 524, 1, 145, 1870, 16, 40, 7, 28, 7, 1, 597, 3, 20, 132, 524, 6, 44, 1, 88, 119, 6, 6682, 240, 37, 5109, 2, 422, 4, 16761, 851, 17, 8995, 3028, 2, 4338, 19157, 6, 39, 5209, 207, 72, 2728, 5, 336]\n",
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    74    11     1   610  6395     8   176   485    13   364     6  6071\n",
      "     1    10    60   438    27    69     3    39  1835     6    74     5\n",
      "   135    72   678 16703   145   119     3    20   524     1   145  1870\n",
      "    16    40     7    28     7     1   597     3    20   132   524     6\n",
      "    44     1    88   119     6  6682   240    37  5109     2   422     4\n",
      " 16761   851    17  8995  3028     2  4338 19157     6    39  5209   207\n",
      "    72  2728     5   336]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train[0])\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "29ff082b690cc6ef49136732bc12e6adf95ef693"
   },
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer.texts_to_sequences(test_sentences)\n",
    "X_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "c367ba4af4d31d0af1a350bef7342765f979890d"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../input/glovetwitter100d/glove.twitter.27B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "03da96d2465ae82506564b2179e44bc4dce7e6f0"
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "d80da73ef79b38fd7dde68cacc3b19df5f4922d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "#change below line if computing normal stats is too slow\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) #embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "91329150363b5ff70ce494c6fabd1b68ffbb7b8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "del tokenized_test, tokenized_train, tokenizer, train_sentences, test_sentences, word_index, embeddings_index, all_embs, nb_words\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "2d23cba7120d3b8c206801fa19e8754c6de44dc9"
   },
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "epochs = 7\n",
    "embed_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "b1c413e9230e1b81c2082b651cd2e90fa9a6bfaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "5c36d39a2094cbaa7e306af22cd870e986f473ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 100)     2000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 100, 100)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 98, 200)      60200       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 96, 200)      120200      conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 94, 256)      153856      conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 92, 256)      196864      conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 45, 512)      393728      conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        (None, 512)          2101248     conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        (None, 512)          2101248     conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1024)         0           cu_dnnlstm_1[0][0]               \n",
      "                                                                 cu_dnnlstm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           65600       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 64)           0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            65          dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,193,009\n",
      "Trainable params: 7,193,009\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def cudnnlstm_model(conv_layers = 2, max_dilation_rate = 3):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "    x = Dropout(0.25)(x)\n",
    "    x = Conv1D(2*embed_size, kernel_size = 3)(x)\n",
    "    prefilt = Conv1D(2*embed_size, kernel_size = 3)(x)\n",
    "    x = prefilt\n",
    "    for strides in [1, 1, 2]:\n",
    "        x = Conv1D(128*2**(strides), strides = strides, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_size=3, kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n",
    "    x_f = CuDNNLSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)  \n",
    "    x_b = CuDNNLSTM(512, kernel_regularizer=l2(4e-6), bias_regularizer=l2(4e-6), kernel_constraint=maxnorm(10), bias_constraint=maxnorm(10))(x)\n",
    "    x = concatenate([x_f, x_b])\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['binary_accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "cudnnlstm_model = cudnnlstm_model()\n",
    "cudnnlstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "0cc9437fc6b557e4fcfca155abb10c00e0935798"
   },
   "outputs": [],
   "source": [
    "weight_path=\"early_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "callbacks = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "e26f4765c718fd8f57773cf8725487cb79a68d40"
   },
   "outputs": [],
   "source": [
    "#cudnnlstm_model.fit(X_train, train_labels, batch_size=batch_size, epochs=epochs, shuffle = True, validation_split=0.20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "6197ef311e5644424397a66fb5dd287f9040d4c3"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'early_weights.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b99a8804e339>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcudnnlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudnnlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test score:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1157\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'early_weights.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "cudnnlstm_model.load_weights(weight_path)\n",
    "score, acc = cudnnlstm_model.evaluate(X_test, test_labels, batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# serialize model to JSON\n",
    "model_json = cudnnlstm_model.to_json()\n",
    "with open(\"cudnnlstm_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "cudnnlstm_model.save_weights(\"cudnnlstm_model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__output__.json', '__notebook__.ipynb', 'cudnnlstm_model.json', 'cudnnlstm_model.h5']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loaded_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-3aabc395a7af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m loaded_model.compile(loss='binary_crossentropy',\n\u001b[0m\u001b[1;32m      2\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   metrics=['binary_accuracy'])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loaded_model' is not defined"
     ]
    }
   ],
   "source": [
    "loaded_model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['binary_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-37-a9f33d5bf18a>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-a9f33d5bf18a>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    loaded_model.predict( sequence.pad_sequences(tokenizer.texts_to_sequences(X_test[0],maxlen=maxlen))\u001b[0m\n\u001b[0m                                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "#print(tokenized_test[0])\n",
    "loaded_model.predict( sequence.pad_sequences(tokenizer.texts_to_sequences(X_test[0],maxlen=maxlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "The logesstic regression model gave accuracy of 90% wherease the CuDNNLSTM gave accuracy of 93%. The deep learning model using GPU to perform sentiment annalysis and can be trained with more iteration to give higher accuracy than 93%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
